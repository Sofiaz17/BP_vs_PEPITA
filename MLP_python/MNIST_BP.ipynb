{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0787a534",
   "metadata": {},
   "source": [
    "This notebook illustrates how to train Fully Connected models with BP. We train and test the model on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a36dc",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6701be",
   "metadata": {},
   "source": [
    "#### Define Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d192b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models with Dropout\n",
    "class NetFC1x1024DOcust(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()      #mnist\n",
    "        self.fc1 = nn.Linear(28**2,128,bias=True)\n",
    "        self.fc2 = nn.Linear(128,10,bias=True)\n",
    "        # self.fc3 = nn.Linear(64,10,bias=True)\n",
    " \n",
    "        # initialize the layers using the He uniform initialization scheme\n",
    "        #fc1_nin = 32*32*3 # Note: if dataset is MNIST --> fc1_nin = 28*28*1\n",
    "        fc1_nin = 28*28*1\n",
    "        fc1_limit = np.sqrt(6.0 / fc1_nin)\n",
    "        torch.nn.init.uniform_(self.fc1.weight, a=-fc1_limit, b=fc1_limit)\n",
    "        fc2_nin = 128\n",
    "        fc2_limit = np.sqrt(6.0 / fc2_nin)\n",
    "        torch.nn.init.uniform_(self.fc2.weight, a=-fc2_limit, b=fc2_limit)\n",
    "        # fc3_nin = 64\n",
    "        # fc3_limit = np.sqrt(6.0 / fc3_nin)\n",
    "        # torch.nn.init.uniform_(self.fc3.weight, a=-fc3_limit, b=fc3_limit)\n",
    "        \n",
    "\n",
    "    def forward(self, x, do_masks):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        # apply dropout --> we use a custom dropout implementation because we need to present the same dropout mask in the two forward passes\n",
    "        if do_masks is not None:\n",
    "            x = x * do_masks[0]   \n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a76a134c-b5fc-4847-8945-0fed23db4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    print('in train_model')\n",
    "    exp_name = '1--MNIST_BP'\n",
    "    train_epochs = 100\n",
    "    learning_rate = 0.1\n",
    "    print('Learning rate:',learning_rate)\n",
    "    dropout = 0.9\n",
    "    keep_rate = dropout\n",
    "    eta_decay = True # to be removed\n",
    "    decay_scheme = 1\n",
    "    seed = None\n",
    "    dataset = 'mnist'\n",
    "    w_init = 'he_uniform'\n",
    "    # network set-up\n",
    "    learn_type = 'BP' # current options are BP, ERIN\n",
    "    optim_choice = 'mom' # current options are SGD, mom(entum)\n",
    "    batch_size = 64\n",
    "    print('Batch size:',batch_size)\n",
    "    model = 'NetFC1x1024DOcust'\n",
    "    dataset = 'mn'\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # initialize the network\n",
    "    net = NetFC1x1024DOcust()\n",
    "    \n",
    "    #optim_choice = 'SGD'\n",
    "    if optim_choice == 'SGD':\n",
    "        gamma = 0\n",
    "    elif optim_choice == 'mom':\n",
    "        gamma = 0.9\n",
    "    \n",
    "    # load the dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()]) # this normalizes to [0,1]\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "    if optim_choice == 'SGD':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "    elif optim_choice == 'mom':\n",
    "        optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=gamma)\n",
    "    elif optim_choice == 'adam':\n",
    "        optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "    \n",
    "    # # # start = torch.cuda.memory_allocated(device)\n",
    "    # # # print(f\"Starting at start: {start} memory usage as baseline.\")\n",
    "    # # # net.to(device)\n",
    "    # # # after_model =  torch.cuda.memory_allocated (device) - start\n",
    "    # # # print(f\"1: After model to device: {after_model:,}\")\n",
    "    # # # print(\"\")\n",
    "\n",
    "    # Train and test the model\n",
    "    test_accs = []\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "\n",
    "    for epoch in range(train_epochs):  # loop over the dataset multiple times\n",
    "        net.train()\n",
    "\n",
    "        # # learning rate decay\n",
    "        if epoch in [30,60,90]: \n",
    "            learning_rate = learning_rate*0.1\n",
    "        \n",
    "        # loop over batches\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, target = data\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # # # a = torch.cuda.memory_allocated(device)  - start\n",
    "            # # # outputs = net(inputs.to(device),do_masks=None)\n",
    "            # # # b = torch.cuda.memory_allocated(device) - start\n",
    "            # # # print(f\"2: Memory consumed after forward pass (activations stored, depends on batch size): {b:,} change: \", f'{b - a:,}' )  # batch * num layers * hidden_size * 4 bytes per float\n",
    "\n",
    "            outputs = net(inputs,do_masks=None)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)  #returns index of column of max values of every row\n",
    "            total_train += target.size(0) #size of rows of test_lbl\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "            \n",
    "            # # # loss = criterion(outputs, target.to(device))\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # # # c = torch.cuda.memory_allocated(device) - start\n",
    "            # # # print(f\"3: After backward pass (activations released, grad stored) {c:,} change: {c-b:,}\")\n",
    "\n",
    "            # # # optimizer.step()\n",
    "            # # # d = torch.cuda.memory_allocated(device)  - start\n",
    "            # # # print(f\"4: After optimizer step (moments stored at first time): {d:,} change: {d-c:,} \" )\n",
    "         \n",
    "            # print statistics\n",
    "            loss = criterion(outputs, target)\n",
    "            running_loss += loss.item()         #.item() transforms pytorch tensor in python number (values of loss and loss.item() are the same)\n",
    "                                              # running_loss stores loss for every epoch (sum of loss for every batch)\n",
    "            \n",
    "            # # # net.zero_grad()  \n",
    "            # # # e = torch.cuda.memory_allocated(device)  - start\n",
    "            # # # print(f\"5: After zero_grad step (grads released): {e:,} change: {e-d:,} \" )\n",
    "            # # # print(\"\") \n",
    "\n",
    "            batch_count += 1\n",
    "        curr_loss = running_loss / batch_count    # sum of losses until now in the epoch / epochs done -> average loss per batch \n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "        (epoch, batch_count, curr_loss))\n",
    "        train_losses.append(curr_loss)\n",
    "        print('Train accuracy epoch {}: {} %'.format(epoch, 100 * correct_train / total_train))\n",
    "        train_accs.append(100 * correct_train / total_train)\n",
    "\n",
    "        print('Testing...')\n",
    "        net.eval()\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for test_data in testloader:\n",
    "                test_images, test_labels = test_data\n",
    "                # calculate outputs by running images through the network\n",
    "                test_outputs = net(test_images,do_masks=None)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(test_outputs.data, 1)\n",
    "                total_test += test_labels.size(0)\n",
    "                correct_test += (predicted == test_labels).sum().item()\n",
    "\n",
    "        print('Test accuracy epoch {}: {} %'.format(epoch, 100 * correct_test / total_test))\n",
    "        test_accs.append(100 * correct_test / total_test)\n",
    "    \n",
    "    print('Finished Training')\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', color=\"green\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss') \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_accs, label='Test Accuracy', color=\"green\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.yticks(np.arange(min(test_accs)-1, max(test_accs)+2, 1))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(test_accs, label='Test Accuracy', color=\"green\")\n",
    "    plt.plot(train_accs, label='Train Accuracy', color=\"red\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(train_accs, label='Train Accuracy', color=\"red\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67c7f9-f649-4b23-a639-ad27c6f55807",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
