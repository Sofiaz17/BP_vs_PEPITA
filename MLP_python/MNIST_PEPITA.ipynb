{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0787a534",
   "metadata": {},
   "source": [
    "This notebook illustrates how to train Fully Connected models with PEPITA. We train and test the model on CIFAR-10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7a36dc",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import psutil\n",
    "import time\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6701be",
   "metadata": {},
   "source": [
    "#### Define Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d192b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models with Dropout\n",
    "class NetFC784(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28,128,bias=True)\n",
    "        self.fc2 = nn.Linear(128,10,bias=True)\n",
    "        \n",
    "        # initialize the layers using the He uniform initialization scheme\n",
    "        fc1_nin = 28*28 # Note: if dataset is MNIST --> fc1_nin = 28*28*1\n",
    "        fc1_limit = np.sqrt(6.0 / fc1_nin)\n",
    "        torch.nn.init.uniform_(self.fc1.weight, a=-fc1_limit, b=fc1_limit)\n",
    "        fc2_nin = 128\n",
    "        fc2_limit = np.sqrt(6.0 / fc2_nin)\n",
    "        torch.nn.init.uniform_(self.fc2.weight, a=-fc2_limit, b=fc2_limit)\n",
    "        # fc3_nin = 64\n",
    "        # fc3_limit = np.sqrt(6.0 / fc3_nin)\n",
    "        # torch.nn.init.uniform_(self.fc3.weight, a=-fc3_limit, b=fc3_limit)\n",
    "  \n",
    "        \n",
    "\n",
    "    def forward(self, x, do_masks):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        # apply dropout --> we use a custom dropout implementation because we need to present the same dropout mask in the two forward passes\n",
    "        if do_masks is not None:\n",
    "            x = x * do_masks[0] \n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)    #normalizes outputs to probability distribution\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad144c5",
   "metadata": {},
   "source": [
    "#### Set hyperparameters and train+test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a085cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    # set hyperparameters\n",
    "    ## learning rate\n",
    "    learning_rate = 0.1\n",
    "    print('Learning rate:',learning_rate)\n",
    "    ## dropout keep rate\n",
    "    keep_rate = 0.9\n",
    "    ## loss --> used to monitor performance, but not for parameter updates (PEPITA does not backpropagate the loss)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    ## optimizer (choose 'SGD' o 'mom')\n",
    "    optim = 'mom' # --> default in the paper\n",
    "    if optim == 'SGD':\n",
    "        gamma = 0\n",
    "    elif optim == 'mom':\n",
    "        gamma = 0.9\n",
    "    ## batch size\n",
    "    batch_size = 64 # --> default in the paper\n",
    "    print('Batch size: ',batch_size)\n",
    "    #epochs\n",
    "    epochs = 100\n",
    "\n",
    "    # initialize the network\n",
    "    net = NetFC784()\n",
    "\n",
    "    # load the dataset\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor()]) # this normalizes to [0,1]\n",
    "    trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)      \n",
    "    testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    " \n",
    "    # define function to register the activations --> we need this to compare the activations in the two forward passes\n",
    "    activation = {}\n",
    "    def get_activation(name):             \n",
    "        def hook(model, input, output):\n",
    "            activation[name] = output.detach()\n",
    "        return hook\n",
    "    for name, layer in net.named_modules():\n",
    "        layer.register_forward_hook(get_activation(name))\n",
    "\n",
    "\n",
    "    # define B --> this is the F projection matrix in the paper (here named B because F is torch.nn.functional)\n",
    "    nin = 28*28         #B-> 784*10\n",
    "    sd = np.sqrt(6/nin)\n",
    "    B = (torch.rand(nin,10)*2*sd-sd)*0.05  # B is initialized with the He uniform initialization (like the forward weights)\n",
    "    #B = torch.ones((784,10))\n",
    "    \n",
    "    # check cosine similarity before training AND matrix norm\n",
    "    angles = []\n",
    "    w_all = []\n",
    "    norm_w0 = []\n",
    "    for l_idx, (name,w) in enumerate(net.named_parameters()):\n",
    "        if 'bias' in name:\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            w_all.append(copy.deepcopy(w))\n",
    "            if l_idx == 0:\n",
    "                norm_w0.append(torch.norm(w))\n",
    "            print('norm of w at layer {} is {}'.format(l_idx,torch.norm(w)))\n",
    "    w_prod = w_all[0].T\n",
    "    for idx in range(1,len(w_all)):\n",
    "        w_prod = torch.matmul(w_prod,w_all[idx].T)\n",
    "\n",
    "    # do one forward pass to get the activation size needed for setting up the dropout masks\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "   \n",
    "    images = torch.flatten(images, 1) # flatten all dimensions except batch    \n",
    "\n",
    "    outputs = net(images,do_masks=None)\n",
    "    layers_act = []\n",
    "    for key in activation:\n",
    "        if 'fc' in key or 'conv' in key:\n",
    "            layers_act.append(F.relu(activation[key]))        \n",
    "            \n",
    "    # set up for momentum4\n",
    "    if optim == 'mom':\n",
    "        gamma = 0.9\n",
    "        v_w_all = []\n",
    "        for l_idx, (name,w) in enumerate(net.named_parameters()):\n",
    "            if len(w.shape)>1:\n",
    "                with torch.no_grad():\n",
    "                    v_w_all.append(torch.zeros(w.shape))\n",
    "\n",
    "    # # # start = torch.cuda.memory_allocated(device)\n",
    "    # # # print(\"Starting at 0 memory usage as baseline.\")\n",
    "    # # # net.to(device)\n",
    "    # # # after_model =  torch.cuda.memory_allocated(device) - start\n",
    "    # # # print(f\"1: After model to device: {after_model:,}\")\n",
    "    # # # print(\"\")\n",
    "\n",
    "    # Train and test the model\n",
    "    test_accs = []\n",
    "    train_accs = []\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        # learning rate decay\n",
    "        if epoch in [30,60,90]: \n",
    "            learning_rate = learning_rate*0.1\n",
    "            print('learning_rate decreased to ',learning_rate)\n",
    "        \n",
    "        # loop over batches\n",
    "        running_loss = 0.0\n",
    "        batch_count = 0\n",
    "        total_train = 0\n",
    "        correct_train = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, target = data\n",
    "            \n",
    "            inputs = torch.flatten(inputs, 1) # flatten all dimensions except batch\n",
    "           \n",
    "            target_onehot = F.one_hot(target,num_classes=10)\n",
    "            \n",
    "            # create dropout mask for the two forward passes --> we need to use the same mask for the two passes\n",
    "            do_masks = []\n",
    "            if keep_rate < 1:\n",
    "                for l in layers_act[:-1]:\n",
    "                    input1 = l\n",
    "                    do_mask = Variable(torch.ones(inputs.shape[0],input1.data.new(input1.data.size()).shape[1]).bernoulli_(keep_rate))/keep_rate\n",
    "                    do_masks.append(do_mask)        #\n",
    "                do_masks.append(1) # for the last layer we don't use dropout --> just set a scalar 1 (needed for when we register activation layer)\n",
    "                            \n",
    "            # forward pass 1 with original input --> keep track of activations\n",
    "            outputs = net(inputs,do_masks)\n",
    "            \n",
    "            # # # a = torch.cuda.memory_allocated(device)  - start\n",
    "            # # # outputs = net(inputs.to(device),do_masks=None)\n",
    "            # # # b = torch.cuda.memory_allocated(device) - start\n",
    "            # # # print(f\"2: Memory consumed after first forward pass (activations stored, depends on batch size): {b:,} change: \", f'{b - a:,}' )  # batch * num layers * hidden_size * 4 bytes per float\n",
    "\n",
    "\n",
    "              # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)  #returns index of column of max values of every row\n",
    "            total_train += target.size(0) #size of rows of test_lbl\n",
    "            correct_train += (predicted == target).sum().item()\n",
    "              \n",
    "            # # # x = torch.cuda.memory_allocated(device) - start\n",
    "            layers_act = []\n",
    "            cnt_act = 0\n",
    "            for key in activation:\n",
    "                if 'fc' in key or 'conv' in key:\n",
    "                    layers_act.append(F.relu(activation[key])* do_masks[cnt_act]) # Note: we need to register the activations taking into account non-linearity and dropout mask\n",
    "                    cnt_act += 1\n",
    "            # # # y = torch.cuda.memory_allocated(device) - start\n",
    "            # # # print(f\"2: Memory for layers_act: {y:,} change: \", f'{y - x:,}' )  # batch * num layers * hidden_size * 4 bytes per float\n",
    "           \n",
    "            # compute the error\n",
    "            error = outputs - target_onehot\n",
    "            \n",
    "            # modify the input with the error\n",
    "            error_input = error @ B.T\n",
    "            mod_inputs = inputs + error_input\n",
    "            \n",
    "            # forward pass 2 with modified input --> keep track of modulated activations\n",
    "            mod_outputs = net(mod_inputs,do_masks)\n",
    "            \n",
    "            # # # c = torch.cuda.memory_allocated(device) - start         \n",
    "            # # # # forward pass 2 with modified input --> keep track of modulated activations\n",
    "            # # # mod_outputs = net(mod_inputs.to(device),do_masks=None)\n",
    "            # # # d = torch.cuda.memory_allocated(device)  - start\n",
    "            # # # print(f\"4: After second forward pass: {d:,} change: {d-c:,} \" )\n",
    "\n",
    "            # # # j = torch.cuda.memory_allocated(device) - start\n",
    "            \n",
    "            mod_layers_act = []\n",
    "            cnt_act = 0\n",
    "            for key in activation:\n",
    "                if 'fc' in key or 'conv' in key:\n",
    "                    mod_layers_act.append(F.relu(activation[key])* do_masks[cnt_act]) # Note: we need to register the activations taking into account non-linearity and dropout mask\n",
    "                    cnt_act += 1\n",
    "            # # # z = torch.cuda.memory_allocated(device) - start\n",
    "            # # # print(f\"2: Memory for mod_layers_act: {z:,} change: \", f'{z - j:,}' )  # batch * num layers * hidden_size * 4 bytes per float\n",
    "\n",
    "            mod_error = mod_outputs - target_onehot\n",
    "            \n",
    "            # compute the delta_w for the batch\n",
    "            delta_w_all = []        #weight update\n",
    "            v_w = []\n",
    "            for l_idx,w in enumerate(net.parameters()):\n",
    "                v_w.append(torch.zeros(w.shape))\n",
    "\n",
    "                \n",
    "            for l in range(len(layers_act)):    #0,1\n",
    "\n",
    "                # update for the last layer\n",
    "                if l == len(layers_act)-1:  #last layer #1\n",
    "                    \n",
    "                    if len(layers_act)>1:\n",
    "                        delta_w = -mod_error.T @ mod_layers_act[-2]  #weight update (layer before)\n",
    "\n",
    "                    else:\n",
    "                        delta_w = -mod_error.T @ mod_inputs\n",
    "                \n",
    "                # update for the first layer\n",
    "                elif l == 0:\n",
    "                    delta_w = -(layers_act[l] - mod_layers_act[l]).T @ mod_inputs #(x+Fe)\n",
    "\n",
    "                # update for the hidden layers (not first, not last)\n",
    "                elif l>0 and l<len(layers_act)-1:\n",
    "                    delta_w = -(layers_act[l] - mod_layers_act[l]).T @ mod_layers_act[l-1]\n",
    "                \n",
    "                delta_w_all.append(delta_w)\n",
    "\n",
    "            # # # y2 = torch.cuda.memory_allocated(device) - start\n",
    "            # # # print(f\"2: Memory for delta_w_all: {y2:,} change: \", f'{y2 - y1:,}' )\n",
    "                 \n",
    "            # apply the weight change\n",
    "            if optim == 'SGD':\n",
    "                for l_idx,w in enumerate(net.parameters()):\n",
    "                    with torch.no_grad():\n",
    "                        w += learning_rate * delta_w_all[l_idx]/batch_size # specify for which layer\n",
    "                        \n",
    "            elif optim == 'mom':\n",
    "                weight_idx = 0\n",
    "                for l_idx, (name,w) in enumerate(net.named_parameters()):\n",
    "                    if 'bias' in name:\n",
    "                        continue\n",
    "                    with torch.no_grad():\n",
    "                        v_w_all[weight_idx] = gamma * v_w_all[weight_idx] + learning_rate * delta_w_all[weight_idx]/batch_size\n",
    "                        w += v_w_all[weight_idx]\n",
    "                        weight_idx += 1\n",
    "                        \n",
    "            \n",
    "            # keep track of the loss\n",
    "            loss = criterion(outputs, target)\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "\n",
    "        curr_loss = running_loss / batch_count\n",
    "        print('[%d, %5d] loss: %.3f' % (epoch, batch_count, curr_loss))\n",
    "        train_losses.append(curr_loss)\n",
    "        print('Train accuracy epoch {}: {} %'.format(epoch, 100 * correct_train / total_train))\n",
    "        train_accs.append(100 * correct_train / total_train)\n",
    "                        \n",
    "        print('Testing...')\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for test_data in testloader:\n",
    "                test_images, test_labels = test_data\n",
    "                test_images = torch.flatten(test_images, 1) # flatten all dimensions except batch\n",
    "                # calculate outputs by running images through the network\n",
    "                test_outputs = net(test_images,do_masks=None) \n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(test_outputs.data, 1)  #returns index of column of max values of every row\n",
    "                total_test += test_labels.size(0) #size of rows of test_lbl\n",
    "                correct_test += (predicted == test_labels).sum().item()\n",
    "\n",
    "        print('Test accuracy epoch {}: {} %'.format(epoch, 100 * correct_test / total_test))\n",
    "        test_accs.append(100 * correct_test / total_test)\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Training Loss', color=\"green\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss') \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_accs, label='Test Accuracy', color=\"green\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.yticks(np.arange(min(test_accs)-1, max(test_accs)+2, 1))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(test_accs, label='Test Accuracy', color=\"green\")\n",
    "    plt.plot(train_accs, label='Train Accuracy', color=\"red\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    #plt.yticks(np.arange(min(train_accs)-1, max(train_accs)+2, 1))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(train_accs, label='Train Accuracy', color=\"red\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Train accuracy (%)')\n",
    "    #plt.yticks(np.arange(min(train_accs)-1, max(train_accs)+2, 1))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
